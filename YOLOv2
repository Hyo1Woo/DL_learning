이번에 살펴볼 논문은 1 step Object Detection의 포문을 연 yolo의 후속편 yolo v2입니다. 저자인 Redmon은 상당히 괴짜인데요, 이번 논문의 형식부터 상당히 재미있습니다. 저는 이렇게 첫 장의 절반을 피규어로 채운 논문은 처음 봤습니다 ㅎㅎ 그 밖에도 기존의 논문 형식의 틀을 깨고 각 섹션의 소제목들을 Better, Faster, Stronger로 달았습니다. 


파격적인 형식에 맞게 내용도 상당히 재미있습니다. 기존 yolo 모델을 보완하여 정확도를 높인 yolo v2 모델을 제시합니다. 그리고 이 yolo v2 모델을 기반으로 무려 9000 종류의 물체를 구분할 수 있는 yolo 9000 모델을 공개합니다. 이전까지 Object Detection 분야에서 가장 많이 사용되었던 데이터 셋인 coco가 약 80종류의 클래스를 가진 것과 비교하면 가히 파격적입니다. 그렇다면 이 괴짜 Redmon이 어떻게 새로운 yolo를 만들었는지 알아볼까요?

Better
Better 쳅터에서 저자는 기존 yolo 모델의 한계점으로 지적되었던 정확도를 어떻게 개선하였는 지를 설명합니다. 쭉 정리해보면 좋다는 건 다끌어와 yolo에 때려부었습니다. 

 

· Batch Normalization 적용

· 높은 해상도 이미지로 백본 CNN 네트워크 fine tune
· Anchor Box 개념 적용하여 학습 안정화

· 높은 해상도의 피쳐맵을 낮은 해상도 피쳐맵에 합치기

 

1. Batch Normalization의 적용

기존 모델에서 Dropout Layer를 제거하고 Batch Normalization을 추가합니다. 이를 통해 mAP가 2% 증가합니다.

 

2. High Resolution Classifier

기존 yolo 모델은 224x224 크기의 해상도로 학습된 VGG 모델을 가져온 다음, 448x448 크기의 이미지에 대해서 Object Detection을 수행하게끔 구성되어 있어 해상도가 맞지 않았습니다. 이를 Object Detection 학습 전에 Image Classification 모델을 큰 해상도 이미지에 대해서 fine-tuning 함으로써 해결하였으며, 약 4% mAP가 증가했다고 합니다.

 

3. Convolutional With Anchor Boxes
![3.con](./image/3.con.png)

다음으로 기존 yolo에서 Fully Connected Layer를 떼어내고 Fully Convolutional Network 형태로 prediction을 계산합니다. 또한 앵커 박스의 개념을 도입합니다. 아래 기존 yolo 구조를 보시면 FC Layer를 두번 거쳐서 최종적으로 7x7x30 크기의 피쳐맵을 얻어냅니다. 7x7은 입력 이미지를 그리드 단위로 나눈 것이고 각 그리드 별 30차원 벡터는 5차원 벡터로 표기된 박스 두개와 20개의 클래스에 대한 스코어 값을 합친 것입니다.


여기서 중요한 점은 5차원 박스를 예측할 때 (중심점 x,y 좌표, 박스 너비 높이 w, h, 물체일 확률 p) 이렇게 다섯가지 정보를 합친 벡터를 사용했다는 것입니다. 이는 사전에 박스는 어떠한 형태일 것이다라는 사전 정보 없이 그냥 박스를 prediction 하는 것입니다. 때문에 예측하는 박스의 크기나 위치가 중구난방이 될 우려가 있습니다. yolo v2에서는 앵커박스의 개념을 도입합니다. 

 

4. Dimension Cluster

앵커 박스란 Faster R-CNN에서 등장한 개념으로 생소하신 분들은 이전 포스팅을 참고해주세요. 핵심은 사전에 크기와 비율이 모두 결정되어 있는 박스를 전제하고, 학습을 통해서 이 박스의 위치나 크기를 세부 조정하는 것을 말합니다. 아예 처음부터 중심점의 좌표와 너비, 높이를 결정하는 방식보다 훨씬 안정적으로 학습이 가능합니다. 앵커 박스는 적당히 직관적인 크기의 박스로 결정하고, 비율을 1:2, 1:1, 2:1로 설정하는 것이 일반적이었습니다만, yolo v2는 여기에 learning algorithm을 적용합니다.

 


바로 coco 데이터 셋의 바운딩 박스에 K-means clustering을 적용하였습니다. 그 결과 앵커 박스를 5개로 설정하는 것이 precision과 recall 측면에서 좋은 결과를 낸다고 결론짓습니다.

 

5.Direct Location Prediction

 이렇게 결정한 앵커 박스에 따라서 하나의 셀에서 5차원 벡터로 이루어진 바운딩 박스를 예측합니다. 박스의 차원은 같지만 담고있는 정보는 다른데요, (tx, ty, tw, th, to)를 학습을 통해서 예측하게 되며, 이를 아래와 같은 방식을 적용하여 바운딩 박스를 구합니다.

 


기존의 yolo가 그리드의 중심점을 예측했다면, yolov2에서는 left top 꼭지점으로부터 얼만큼 이동하는 지를 예측합니다. 이것이 bx=σ(tx) + cx가 의미하는 바입니다. 다음으로 너비와 높이는 사전에 정의된 박스의 크기를 얼만큼 비율로 조절할 지를 지수승을 통해 예측하며, bw=pwe^tw에 잘 나타나 있습니다. bounding box regression과 관련한 더 자세한 내용은 R-CNN에 관련한 이전 포스팅을 참고해주세요. 이러한 앵커 박스의 적용을 통해서 mAP가 5% 정도 상승했다고 합니다.

 

6. Fine-Grained Features


기존의 yolo에서는 CNN을 통과한 마지막 레이어의 피쳐맵만 사용하여 작은 물체에 대한 정보가 사라진다는 비판이 있었습니다. yolo v2에서는 상위 레이어의 피쳐맵을 하위 피쳐맵에 합쳐주는 passthrough layer를 도입하였습니다. 위 그림에서 볼 수 있듯이 높은 해상도를 가진 26x26x256 피쳐맵을 13x13x2048 크기로 리스케일하여 낮은 해상도의 피쳐맵과 합쳐 13x13x3072 크기의 피쳐맵을 만들어 내는  모습을 볼 수 있습니다.

 

7. Multi-Scale Training

작은 물체들을 잘 잡아내기 위해서 yolov2는 여러 스케일의 이미지를 학습할 수 있도록 하였습니다. Fully Connected Layer를 떼어냈기 때문에 입력 이미지의 해상도에서 비교적 자유로울 수 있게 되었습니다. yolo v2는 이를 활용하여 학습 시에 {320, 352, ..., 608} 와 같이 32 픽셀 간격으로 매 10 배치시마다 입력 이미지의 해상도를 바꿔주며 학습을 진행합니다. 

 

결과


쭉 훑어보면 기존에 좋다고 알려진 기법들을 싹다 모아서 yolo 모델에 적용한 것을 알 수 있습니다. 그 결과로 기존의 yolo를 훨씬 상회하는 정확도 높은 모델을 만들어 냅니다. 아키텍쳐의 큰 변화 없이 엔지니어링 만으로 이러한 변화를 끌어냈다는 것이 흥미롭네요.

Faster
이 쳅터에서는 yolo v2가 yolo v1보다 속도 측면에서 어떤 개선을 이루었는지 설명합니다. 핵심은 기존의 yolo가 pretrained VGG 혹은 Googlenet을 사용하였는데 이 백 본 네트워크가 너무 크고 복잡하다는 것입니다. 이에 저자는 직접 새로운 CNN 아키텍쳐인 Darknet을 제시합니다. Darknet의 구조는 아래와 같습니다.


전체 구조는 VGG와 크게 다르지 않습니다만 MaxPooling을 줄이고 컨볼루션 연산을 늘리는 것이 눈에 띕니다. 또한 마지막 단에 Fully Connected Layer를 제거하고 Convolution 연산으로 대체하여 파라미터의 수를 줄인 것을 볼 수 있습니다. 이러한 경량 CNN 아키텍쳐를 사용하여 속도 측면에서도 개선을 이루었습니다.

Stronger
이 논문에서 가장 재미있는 부분입니다. 바로 yolo v2 아키텍쳐를 기반으로 총 9000개에 달하는 클래스를 잡아내는 yolo v2를 어떻게 학습시켰는 지를 설명합니다. 

 

Hierarchical Classification


먼저 저자는 방대한 크기의 class에 대해서 classification을 수행할 경우 계층적으로 분류 작업을 수행해야한다고 제시합니다. 가령 이미지 넷 데이터를 보면 개과 안에 웰시코기니, 요크 셔테리어와 같은 라벨들이 속합니다. 이 점에 착안하여 저자는 Softmax 연산을 수행할 때 전체 클래스에 대해서 한번에 수행하지 말고, 각 대분류 별로 Softmax를 수행할 것을 제안합니다.

 

Dataset combination with WordTree

이러한 계층 구조를 적극 도입하여 저자는 coco와 imagenet 데이터 셋의 라벨을 트리 구조를 활용하여 섞습니다.


Joint classification and detection

대망의 학습 부분입니다. 앞서 wordtree를 이용하여 총 9418개의 클래스를 가진 데이터 셋을 만들어 냈습니다. (ImageNet Classification + COCO) 그러나 이 중 9000개의 클래스는 ImageNet Classification 데이터 셋에 속하였고, 이들은 Classification 라벨만 붙어있는 상태입니다.

 

저자는 학습 과정에서 COCO 데이터 셋이 더 많이 샘플링 되도록 하여 실제 모델이 학습하는 이미지의 비율을 4:1로 맞춰주었습니다. 그리고 Classification 라벨만 붙어있는 이미지의 경우에는 Classification Loss만 역전파가 되게끔 하였습니다. 이를 통해서 Classification과 Object Detection 테스크가 섞여있는 데이터 셋을 학습시킬 수 있게 되었습니다.

 

결과는 상당히 흥미롭습니다. 저자는 학습시킨 yolo 9000을 ImageNet Detection Challenge 데이터 셋을 활용하여 성능 평가를 진행하였고, 19.7 mAP를 얻습니다. 특히 detection 라벨이 붙은 데이터를 하나도 학습하지 못한 156개의 클래스에 대해서는 16.0 mAP라는 정확도를 달성합니다. 수치만 놓고 보면 실망스러울 수 있지만, 모델이 무려 9000개의 클래스를 구분하도록 학습을 진행헀다는 것을 생각해보면 놀라운 결과라는 생각이 듭니다.

마치며
yolo 시리즈의 저자 Redmon의 논문을 보면 항상 틀을 깨는 창의력을 보여줍니다. 이번 논문에서도 누구도 시도한 적 없었던 클래스 수를 9000개까지 늘리는 모습을 보여줍니다. 일반적인 연구자였다면 성능을 끌어올리는 정도에서 멈췄겠죠? 이러한 혁신적인 연구자인 Redmon이 최근 자신의 연구 결과가 전쟁 기술 개발에 적용된다는 사실에 비젼 연구를 그만둔다고 합니다. 개인적으로 안타까우면서도 연구자들이 함께 고민해봐야할 문제가 아닐까 싶네요.

 

다음에는 이 yolo v2를 다시 업그레이드 한 yolo v3의 리뷰로 돌아오겠습니다.

 

감사합니다.
